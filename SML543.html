# PHI543-sp23

# sarahjaneleslie.github.io
<b>SYLLABUS - Spring 2023</b>

<H1>PHI 543/SML 543<br>
Machine Learning: A Practical Introduction for Humanists and Social Scientists</H1>
<H3>Sarah-Jane Leslie<br>
sjleslie@princeton.edu<br>
	<a href="www.princeton.edu/leslie">www.princeton.edu/leslie</a></H3>

<H3> Thursdays 10:30-1:20, rm. 103 Bendheim House (CSML), 26 Prospect Ave</H3>


<p>Machine learning – and in particular, deep learning – is rapidly opening new horizons for research in the humanities and social sciences. However, scholars in the humanities and social sciences can encounter barriers to learning about such techniques – for example, machine learning courses, especially at the graduate level, often require multivariate calculus, linear algebra and prior coding experience, which students in the humanities and less quantitative social sciences may lack. This course offers a practical introduction to deep learning for graduate students, without assuming knowledge of calculus or other college-level math, or any prior experience with coding. By the end of the course, students will:</p> 

<ul><li>be able to code and train a variety of basic deep learning models</li>
<li>develop an appreciation of the range of humanities/social science research questions to which deep learning can be applied</li>
<li>be fluent collaborators on research projects that involve machine learning experts</li>
<li>gain an understanding than will inform theorizing about machine learning (e.g., for research in AI ethics, technology policy, etc)</li></ul>

<p><b>Readings:</b> The primary text for the course is <em>Francois Chollet, 2021, Deep learning with Python. Manning.</em> We will work our way through the book in a lot of detail. In the second half of the course, once we have worked through the basics, there are supplemental readings each week, which give examples of how related models have been used in humanities/social science research. The supplemental readings aim to provide a sense of the breadth of applications, as well as to raise interesting issues for further thought and consideration.</p>

<p><b>Evaluation and assignments:</b> There are simple weekly coding assignments to cement knowledge and build skills. Students taking the course for credit are required to complete these, though they are ungraded. That is, as long as a reasonable effort is made on them, students receive full credit for the week. In addition, there are three graded projects spaced throughout the semester. For these projects, students will be supplied with a data set, or list of data sets, and asked to build a model that addresses a question appropriate to the data set. Evaluation will be based on the appropriateness of the model for the question and the success of the code used to build/train the model, as well as answers to any other questions posed along the way. The three projects will cover the following areas respectively: classification/regression, computer vision, natural language processing. (For philosophy students only: successful completion of the above assignments will constitute a logic unit. Philosophy students wishing to receive a unit other than logic should contact me directly.) </p> 

<p>Final grades are determined in the following way: 20% homework, 20% classification/regression assignment, 30% computer vision assignment, 30% natural language processing assignment. </p>

<p><b>Auditing:</b> Graduate students are welcome to audit this course, however I request that even auditors engage with the weekly coding assignments, since the value of this class is largely associated with the development of practical skills. As with any kind of coding, these skills can only be learned by doing. Post-docs and other researchers/faculty are welcome to informally audit the class also.</p>


<H3>Important: Getting started in Python</H3>

<p>I <b>highly recommend</b> that students who have never coded before do a Python tutorial prior to the first class. A repository of good tutorials can be found <a href="https://researchcomputing.princeton.edu/external-online-resources/python">here</a>. For the <a href="https://www.w3schools.com/python/">W3 Schools tutorial,</a> I recommend working through it up to Classes/Objects. You might also consider attending a 2023 Wintersession course such as Intro to Programming Using Python or Python for Poets.</p>

<p>I also recommend taking a look at <a href="https://pythontutor.com/python-debugger.html#mode=edit">this resource</a> for visualizing and understanding the execution of programs in Python.</p>

<p>We will spend much of the first session doing a crash course in Python, but coding is a skill that takes time to develop. The first class will be very difficult for you if you are seeing the material for the very first time.</p>

<p>To get started writing Python code, you first need to <a href="https://forms.rc.princeton.edu/registration/?q=adroit">create an Adroit account.</a> Once you have done so, you can simply go <a href="https://myadroit.princeton.edu/">here</a> and navigate to Interactive Apps then Jupyter for Classes. (Note that VPN is required from off campus.) You do not need to install anything locally on your computer. More detailed instructions can be found on the Canvas page for this course.</p>

<H3>Schedule of Topics -- updated 2/17</H3>

<p><b>Week 1:</b> Overview of machine learning; crash course in Python basics<br>
Reading: Chollet, chapter 1</p>

<p><b>Week 2:</b> Fundamentals of neural networks: the forward pass<br>
Reading: Chollet, chapter 2</p>

<p><b>Week 3:</b> Fundamentals of neural networks: the backward pass<br>
Reading: Chollet, chapter 2 cont.</p>

<p><b>Week 4:</b> Introduction to Keras;  Classification and regression<br>
Reading: Chollet, sections 3.1-3.3, 3.6, & 4.1-4.2</p>

<p><b>Week 5:</b> Training, validation, and test sets; regularization; other practicalities<br>
Reading: Chollet, section 4.3 & chapter 5</p>

<p><b>Week 6:</b> Best practices for model evaluation; overview of other helpful Python libraries<br>
Reading: Chollet, chapter 6, <a href="https://arxiv.org/abs/2207.07048">Kapoor, S. & Narayanan, A. (2022)</a> </p>

<em>Classification/regression project assignment distributed</em>

<p>Spring break</p>

<p><b>Week 7:</b> The Keras functional API; loose ends; introduction to computer vision<br>
Reading: Chollet, chapter 7 up to (but not including) section 7.2.3<br>
Supplemental reading:  </p>

<em>Classification/regression project due</em>

<p><b>Week 8:</b> Computer vision I: introduction to convolutional neural networks<br>
Reading: Chollet, sections 8.1-8.2<br>
Supplemental reading: <a href="https://academic.oup.com/dsh/article/35/1/194/5296356">Melvin Wevers, Thomas Smits, The visual digital turn: Using neural networks to study historical images, Digital Scholarship in the Humanities, Volume 35, Issue 1, April 2020, Pages 194–207.</a></p>

<p><b>Week 9:</b> Computer vision II: classification; pretrained models<br>
Reading: Chollet, sections 8.3; 9.3<br>
Supplemental reading: <a href="https://www.sciencedirect.com/science/article/pii/S0305440321000455">Leszek M. Pawlowicz, Christian E. Downum, Applications of deep learning to decorated ceramic typology and classification: A case study using Tusayan White Ware from Northeast Arizona, Journal of Archaeological Science, Volume 130, 2021, 105375.</a></p>

<em>Computer vision project assignment distributed</em>

<p><b>Week 10:</b> Natural Language Processing I: representing words as numbers<br>
Reading: Chollet sections 11.1-11.3<br>
Supplemental reading: <a href="https://tessaescharlesworth.files.wordpress.com/2022/07/charlesworth_hist-embeddings_published.pdf">Charlesworth, T.E.S., Caliskan, A., & Banaji, M.R., (2022) Historical representations of social groups across 200 years of word embeddings from Google Books. Proceedings of the National Academy of Sciences, 119(28).</a><br> 
<a href="https://www.nature.com/articles/s41562-022-01316-8">Grand, G., Blank, I.A., Pereira, F. et al. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. Nat Hum Behav 6, 975–987 (2022).</a> <br>




<p><b>Week 11:</b> NLP II: transformers<br>
Reading: Chollet chapter 11.4<br>
Supplemental reading: <a href="https://www.nature.com/articles/s41586-022-04448-z?utm_campaign=The%20Batch&utm_medium=email&_hsmi=222428230&_hsenc=p2ANqtz-_kr0H_rGJOIJRXWpDlGwP298BuR5SoKbROhB05hKXLpwXYaktKDQq3fq7RfNIXxV4DSCytHhkc_T2aCIlnx-SWBivzg1GfDrQFM5c4bz-0KgE_Low&utm_content=222428230&utm_source=hs_email">Assael, Y., Sommerschield, T., Shillingford, B. et al. Restoring and attributing ancient texts using deep neural networks. Nature 603, 280–283 (2022).</a> </br>
<a href="https://www.pnas.org/doi/10.1073/pnas.1907367117">Manning, C., Clark, K., Hewitt, J., & Levy, O. Emergent linguistic structure in artificial neural networks trained by self-supervision. PNAS 117 (48).</a></p>

<em>Computer vision project due</em><br>

<em>NLP project distributed</em><br>


<p><b>Week 12:</b> NLP III: generative models; loose ends<br>
Reading: Chollet chapter 11.5, 12.1<br>
Supplemental reading: <a href="https://www.scientificamerican.com/article/we-asked-gpt-3-to-write-an-academic-paper-about-itself-mdash-then-we-tried-to-get-it-published/">Osmanovic Thunström, A. (2022). We Asked GPT-3 to Write an Academic Paper about Itself—Then We Tried to Get It Published. Scientific American, June 30 2022.</a> <br>
<a href="https://hal.archives-ouvertes.fr/hal-03701250/document"> Gpt Generative Pretrained Transformer, Almira Osmanovic Thunström, Steinn Steingrimsson. Can GPT-3 write an academic paper on itself, with minimal human input?. 2022. hal-03701250.</a><br>
<a href="https://www.nature.com/articles/d41586-021-00530-0">Hutson, M. (2021). Robo-writers: the rise and risks of language-generating AI. Nature 591, 22-25.</a></p>
<p>Newly added: <a href="https://www.nature.com/articles/d41586-023-00340-6">Stokel-Walker & Van Noorden (2023). What ChatGPT and generative AI mean for science. Nature.</a><br>
	<a href="https://www.nature.com/articles/d41586-023-00528-w"> Tregoning, J. (2023). AI writing tools could give scientists the gift of time. Nature.</a><br>
	<a href="https://www.nature.com/articles/d41586-023-00288-7">van Dis, E. et al. (2023). ChatGPT: Five priorities for research. Nature.</a>
</p>

Additional supplemental readings: <a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-like-people/A9535B1D745A0377E16C590E14B94993">Lake, B., Ullman, T., Tenenbaum, J., & Gershman, S. (2017). Building machines that learn and think like people. Behavioral and Brain Sciences, 40, E253.</a> <br>
	<a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/98F5E24BEADE585050B773D2CBEB1F39/S0045509121000230a.pdf/whats_wrong_with_automated_influence.pdf">Benn, C., & Lazar, S. (2022). What’s wrong with Automated Influence. Canadian Journal of Philosophy, 1-24.</a></p>


<em>NLP project due on Dean's date: May 9th</em>
