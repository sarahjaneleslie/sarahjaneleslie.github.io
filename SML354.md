---
layout: page
title: "SML 354"
permalink: /SML354
---



## Artificial Intelligence: A hands-on introduction from basics to ChatGPT

<p><b>Draft syllabus - Spring 2024</b></p>
<p>Sarah-Jane Leslie<br>
sjleslie@princeton.edu<br>
	<a href="www.princeton.edu/leslie">www.princeton.edu/leslie</a></p>

<p>We are in the midst of an AI revolution, but gaining a solid understanding of this technology can be challenging for students without strong mathematical and computational backgrounds. This course offers an introduction to deep learning, which is the core technology behind most modern AI applications (including ChatGPT), aimed at students with little to no coding experience and no mathematical background beyond basic differential calculus. Emphasis will be placed on gaining a conceptual understanding of deep learning models, and on gaining the basic coding skills required to use them in simple contexts. By the end of the course, students will be able to understand, code and train a variety of basic deep learning models, including basic neural nets, image recognition models, and natural language processing models. As a capstone, students will complete a structured project in which they will build their own (tiny) GPT-style text generator.</p>

<p><b>Readings:</b> The primary text for the course is <em>Francois Chollet, 2021, Deep learning with Python. Manning.</em> We will work our way through the book in a lot of detail. In the second half of the course, once we have worked through the basics, there will be supplemental readings assigned as applicable.</p>

<p><b>Evaluation and assignments:</b> Students are required to complete weekly coding assignments and a final structured  project (coding and training a tiny GPT-style text generator). In addition, there are in-class midterm and final exams that test conceptual understanding of course content.</p> 

<p>Final grades are determined in the following way: 25% homeworks, 25% midterm, 25% final, 15% final project, 10% class/precept participation. </p>

<p><h3>Prerequisites</h3> </p>

<p><b>Please note:</b> Students who satisfy the math and computer science requirements (MAT 103, 202 & COS 226) for COS 324: Introduction to Machine Learning should take that course instead. This course is specifically aimed at students who do not meet those requirements and is correspondingly less rigorous in its treatment of the subject. No credit will be given to students who have already completed COS 324 or equivalent. Please check with me if you have any questions about your eligibility. </p>

<p><b>Math prerequisites:</b> <a href="https://www.math.princeton.edu/undergraduate/placement/mat100">MAT 100: Calculus Foundations</a> or equivalent high school course. In particular, students should be familiar with: the concept of a derivative; logarithms and exponents; basic trigonometry, especially cosines; graphs of functions. I do my very best to explain the mathematical concepts that are needed to understand deep learning as we go, but I will assume basic knowledge of those concepts in particular.</p>

<p><b>Coding prerequisities:</b> Prior coding experience is not required for this class, but students without Python experience will need to complete a crash course in the language. In particular, I have made video lectures covering Python basics which will be available via Canvas upon registering for the class. The first several homeworks will assume that you have watched certain portions of the crash course, so you can do this concurrently with the start of classes, <b>however</b> if you are new to coding I strongly recommend getting started in January so you can work at your own pace and do additional practice as needed. The videos have accompanying code notebooks with exercises. If you can complete the exercises in the notebooks, that means you are on a good track. Students with prior experience in Python should test their understanding by making sure they can easily complete those exercises.</p>

<p>Please note that the video tutorial does not cover all aspects of Python; rather the focus is on the key aspects of the language required for the course.</p>

<p><b>Additional resources:</b> A repository of good tutorials can be found <a href="https://researchcomputing.princeton.edu/external-online-resources/python">here</a>. For the <a href="https://www.w3schools.com/python/">W3 Schools tutorial,</a> I recommend working through it up to Classes/Objects. You might also consider attending a Wintersession course on Python.</p>


<H3>Schedule of Topics</H3>

<p><b>Week 1:</b> Overview of machine learning<br>
Reading: Chollet, chapter 1</p>

<p><b>Week 2:</b> Fundamentals of neural networks: the forward pass<br>
Reading: Chollet, chapter 2</p>

<p><b>Week 3:</b> Fundamentals of neural networks: the backward pass<br>
Reading: Chollet, chapter 2 cont.</p>

<p><b>Week 4:</b> Introduction to Keras;  Classification and regression<br>
Reading: Chollet, sections 3.1-3.3, 3.6, & 4.1-4.2</p>

<p><b>Week 5:</b> Training, validation, and test sets; regularization; other practicalities<br>
Reading: Chollet, section 4.3 & chapter 5</p>

<p><b>Week 6:</b> The Keras functional API; Review; Midterm Exam<br>
Reading: Chollet, chapter 7 up to (but not including) section 7.2.3<br></p>


<p>break</p>


<p><b>Week 7:</b> Computer vision I: introduction to convolutional neural networks<br>
Reading: Chollet, sections 8.1-8.2<br>


<p><b>Week 8:</b> Computer vision II: classification; pretrained models<br>
Reading: Chollet, sections 8.3; 9.3<br>


<p><b>Week 9:</b> Natural Language Processing I: representing words as numbers<br>
Reading: Chollet sections 11.1-11.3<br>
Supplemental reading: <a href="https://tessaescharlesworth.files.wordpress.com/2022/07/charlesworth_hist-embeddings_published.pdf">Charlesworth, T.E.S., Caliskan, A., & Banaji, M.R., (2022) Historical representations of social groups across 200 years of word embeddings from Google Books. Proceedings of the National Academy of Sciences, 119(28).</a><br> 




<p><b>Week 10:</b> NLP II: transformers<br>
Reading: Chollet chapter 11.4<br>
Supplemental reading:
<a href="https://www.pnas.org/doi/10.1073/pnas.1907367117">Manning, C., Clark, K., Hewitt, J., & Levy, O. Emergent linguistic structure in artificial neural networks trained by self-supervision. PNAS 117 (48).</a></p>



<p><b>Week 11:</b> NLP III: generative models; loose ends<br>
Reading: Chollet chapter 11.5, 12.1<br>
Supplemental reading: <a href="https://www.scientificamerican.com/article/we-asked-gpt-3-to-write-an-academic-paper-about-itself-mdash-then-we-tried-to-get-it-published/">Osmanovic Thunström, A. (2022). We Asked GPT-3 to Write an Academic Paper about Itself—Then We Tried to Get It Published. Scientific American, June 30 2022.</a> <br>
<a href="https://hal.archives-ouvertes.fr/hal-03701250/document"> Gpt Generative Pretrained Transformer, Almira Osmanovic Thunström, Steinn Steingrimsson. Can GPT-3 write an academic paper on itself, with minimal human input?. 2022. hal-03701250.</a><br>
<a href="https://www.nature.com/articles/d41586-021-00530-0">Hutson, M. (2021). Robo-writers: the rise and risks of language-generating AI. Nature 591, 22-25.</a></p>
<p>Newly added: <a href="https://www.nature.com/articles/d41586-023-00340-6">Stokel-Walker & Van Noorden (2023). What ChatGPT and generative AI mean for science. Nature.</a><br>
	<a href="https://www.nature.com/articles/d41586-023-00528-w"> Tregoning, J. (2023). AI writing tools could give scientists the gift of time. Nature.</a><br>
	<a href="https://www.nature.com/articles/d41586-023-00288-7">van Dis, E. et al. (2023). ChatGPT: Five priorities for research. Nature.</a>
</p>

<p><b>Week 12: Review and loose ends</b></p>

Additional supplemental readings: <a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-like-people/A9535B1D745A0377E16C590E14B94993">Lake, B., Ullman, T., Tenenbaum, J., & Gershman, S. (2017). Building machines that learn and think like people. Behavioral and Brain Sciences, 40, E253.</a> <br>
	<a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/98F5E24BEADE585050B773D2CBEB1F39/S0045509121000230a.pdf/whats_wrong_with_automated_influence.pdf">Benn, C., & Lazar, S. (2022). What’s wrong with Automated Influence. Canadian Journal of Philosophy, 1-24.</a></p>


<em>NLP project due on Dean's date: May 9th</em>
