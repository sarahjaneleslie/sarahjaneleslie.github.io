---
layout: page
title: "SML 354"
permalink: /SML354
---



## Artificial Intelligence: A hands-on introduction from basics to ChatGPT

<p><b>Next offered: Spring 2026</b>
</p>
<p>Sarah-Jane Leslie<br>
sjleslie@princeton.edu<br>
	<a href="https://sarahjaneleslie.org">sarahjaneleslie.org</a></p>

<p>We are in the midst of an AI revolution, but gaining a solid understanding of this technology can be challenging for students without strong mathematical and computational backgrounds. This course offers an introduction to deep learning, which is the core technology behind most modern AI applications (including ChatGPT), aimed at students with little to no coding experience and no mathematical background beyond basic differential calculus. Emphasis will be placed on gaining a conceptual understanding of deep learning models and on practicing the basic coding skills required to use them in simple contexts. By the end of the course, students will be able to understand, code and train a variety of basic deep learning models, including basic neural nets, image recognition models, and natural language processing models. As a capstone, students will complete a structured project in which they will build their own (tiny) GPT-style text generator. Throughout the semester, we will build a rich and nuanced understanding of how AI models operate, what their strengths and limitations are, and correspondly how they can be most effectively deployed across a range of contexts.</p>

<p><b>Readings:</b> The primary text for the course is <em>Francois Chollet & Matthew Watson, 2026, Deep learning with Python, 3rd edition. Manning.</em> We will work our way through the book in detail. At various points in the course, I will post supplemental reading materials to Canvas.</p>

<p><b>Evaluation and assignments:</b> Students are required to complete weekly coding assignments and a final structured  project (coding and training a tiny GPT-style text generator). In addition, there are in-class midterm and final exams that test conceptual understanding of course content.</p> 

<p>Final grades are determined in the following way: 25% homeworks, 25% midterm, 25% final, 15% final project, 10% class/precept participation. </p>

<p><h3>Prerequisites</h3> </p>

<p><b>Please note:</b> Students who satisfy the math and computer science requirements (MAT 103, 202 & COS 226) for COS 324: Introduction to Machine Learning should take that course instead. This course is specifically aimed at students who do not meet those requirements. No credit will be given to students who have already completed COS 324 or equivalent. Please check with me if you have any questions about your eligibility. </p>

<p><b>Math prerequisites:</b> <a href="https://www.math.princeton.edu/undergraduate/placement/mat100">MAT 100: Calculus Foundations</a> or equivalent high school course. In particular, students should be familiar with: the concept of a derivative; logarithms and exponents; basic trigonometry, especially cosines; graphs of functions. I do my very best to explain the mathematical concepts that are needed to understand deep learning as we go, but I will assume basic knowledge of those concepts in particular.</p>

<p><b>Coding prerequisities:</b> Prior coding experience is not required for this class, but students without Python experience will need to complete a crash course in the language. In particular, I have made video lectures covering Python basics which are available via Canvas for anyone with a Princeton netid. To view the videos, please <a href="https://princeton.instructure.com/enroll/KB43PD">self-enroll here.</a><br><br> The first several homeworks will assume that you have watched certain portions of the crash course, so you can do this concurrently with the start of classes, <b>however</b> if you are new to coding I <b>strongly recommend</b> getting started <b>before the beginning of the semester</b> so you can work at your own pace and do additional practice as needed. If you fall behind on this once the semester starts, it may be prohibitively difficult to catch up. The videos have accompanying code notebooks with exercises. If you can complete the exercises in the notebooks, that means you are on a good track. Students with prior experience in Python should test their understanding by making sure they can easily complete those exercises.</p>

<p>Please note that the video tutorial does not cover all aspects of Python; rather the focus is on the key aspects of the language required for the course.</p>

<p><b>Additional resources:</b> A repository of good tutorials can be found <a href="https://researchcomputing.princeton.edu/external-online-resources/python">here</a>. For the <a href="https://www.w3schools.com/python/">W3 Schools tutorial,</a> I recommend working through it up to Classes/Objects. You might also take a look at <a href="https://researchcomputing.princeton.edu/learn/workshops-live-training">Princeton Research Computing's offerings</a>, as they feature mini-courses on Python at various times throughout the year.</p>


<H3>Schedule of Topics (subject to minor timing changes as appropriate)</H3>

<p><b>Week 1:</b> Overview of machine learning; Introduction to neural networks<br>
Reading: Chollet & Watson, chapter 1</p>

<p><b>Week 2:</b> Fundamentals of neural networks I: The forward pass<br>
Reading: Chollet & Watson, sections 2.1-2.3</p>

<p><b>Week 3:</b> Fundamentals of neural networks II: Backpropagation <br>
Reading: Chollet & Watson, 2.4-end of chapter </p>

<p><b>Week 4:</b>  Classification and regression<br>
Reading: Softmax Tutorial and Tasks, Activations, and Losses posted to Canvas. Optional reading on deeper dive on softmax/sigmoid available on Canvas.</p>

<p><b>Week 5:</b> Introduction to Keras; Training, validation, and test sets<br>
Reading: Chollet & Watson, sections 3.1, 3.2, 3.6 (skipping 3.6.1) & 4.1-4.2; section 7.2.2</p>

<p><b>Week 6:</b> Finish any outstanding topics; Review; Begin computer vision, time permitting<br> 
	Midterm Exam held in class on Thursday Oct. 10<br>
Reading: Chollet & Watson, sections 5.2-5.4; chapter 6 (optional) <br></p>


<p>break</p>


<p><b>Week 7:</b> Computer vision I: Introduction to convolutional neural networks<br>
Reading: Chollet & Watson, sections 8.1-8.2; Tutorial on RBG images posted to Canvas<br>


<p><b>Week 8:</b> Computer vision II: classification; dropout regularization; visualizing convnets<br>
Reading: Chollet & Watson, continuing with sections 8.1-8.2;  Optional reading on leveraging pretrained convnets, and on structuring folders in Python, posted to Canvas.<br>


<p><b>Week 9:</b> Introduction to natural language processing: Representing words as numbers<br>
Reading: Reading/tutorial posted to Canvas; <a href="http://jalammar.github.io/illustrated-word2vec/">Jay Alammar, Illustrated Word2Vec</a>Chollet & Watson, 14.5.2, 14.5.4-end<br> Optional: rest of Chollet & Watson chapter 14. Optional social science application of word embeddings posted to Canvas.<br>



<p><b>Week 10:</b> Transformers I: Introduction; Masked language models<br>
Reading: <a href="http://jalammar.github.io/illustrated-transformer/">Jay Alammar, The Illustrated Transformer</a>, Chollet & Watson sections 15.3-15.3.2, 15.3.5, 15.5<br>


<p><b>Week 11:</b> Transformers II: Generative models<br>
Reading: Chollet & Watson chapter 15.3.3<br>


<p><em>Generative transformer project distributed</em></p>

<p><b>Week 12:</b> Transformers III: Generative models cont.; Putting it all together: Review and loose ends<br>
Optional Reading: Chollet & Watson, chapter 19<br>

<p><em>Generative transformer project due on Dean's date</em></p>
<p>Final exam given in class during finals period</p>
