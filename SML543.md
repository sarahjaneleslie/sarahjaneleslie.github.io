---
layout: page
title: "SML 543"
permalink: /SML543
---

## Machine Learning: A practical introduction for humanists and social scientists

<b>Syllabus - Spring 2026</b>


Sarah-Jane Leslie<br>
sjleslie@princeton.edu<br>
	<a href="https://sarahjaneleslie.org">sarahjaneleslie.org</a>


<p>Machine learning – and in particular, deep learning – is rapidly opening new horizons for research in the humanities and social sciences. However, scholars in the humanities and social sciences can encounter barriers to learning about such techniques – for example, machine learning courses, especially at the graduate level, often require multivariate calculus, linear algebra and prior coding experience, which students in the humanities and less quantitative social sciences may lack. This course offers a practical introduction to deep learning for graduate students, without assuming knowledge of calculus or other college-level math, or any prior experience with coding. By the end of the course, students will:</p> 

<ul><li>be able to code and train a variety of basic deep learning models</li>
<li>develop an appreciation of the range of humanities/social science research questions to which deep learning can be applied</li>
<li>be fluent collaborators on research projects that involve machine learning experts</li>
<li>gain an understanding than will inform theorizing about machine learning (e.g., for research in AI ethics, technology policy, etc)</li></ul>

<p>Here is a <a href="https://www.princeton.edu/news/2023/05/02/deep-learning-princetons-graduate-school">Princeton homepage story</a> on the spring 2023 version of the class, which features some helpful student perspectives.</p>

<p><b>Readings:</b> The primary text for the course is <em>Francois Chollet & Matthew Watson, 2026, Deep learning with Python, 3rd edition. Manning.</em> We will work our way through the book in detail. At various points in the course, I will post supplemental reading materials to Canvas.</p>

<p><b>Enrollment eligibility: </b>Open only to Princeton graduate students, except with special permission of the instructor. Princeton undergraduates should take the undergraduate version of this course, <a href="/SML354">SML 354</a></p>

<p><b>Evaluation and assignments:</b> There are simple weekly coding assignments to cement knowledge and build skills. Students taking the course for credit are required to complete these, though they are only lightly graded. (A reasonable effort will typically result in full credit.) In addition, there are three properly graded projects spaced throughout the semester. For these projects, students will be supplied with a data set and asked to build a model that addresses a question appropriate to the data set. Evaluation will be based on the appropriateness of the model for the question and the success of the code used to build/train the model, as well as answers to any other questions posed along the way. The three projects will cover the following areas respectively: classification/regression, computer vision, natural language processing. (For philosophy students only: successful completion of the above assignments will constitute a logic unit. Philosophy students wishing to receive a unit other than logic should contact me directly.) </p> 

<p>Final grades are determined in the following way: 20% homework, 20% classification/regression assignment, 20% computer vision assignment, 30% natural language processing assignment, and 10% class participation. </p>

<p><b>Auditing:</b> Graduate students are welcome to audit this course, however I request that even auditors engage with the weekly coding assignments, since the value of this class is largely associated with the development of practical skills. As with any kind of coding, these skills can only be learned by doing. Post-docs and other researchers/faculty are welcome to informally audit the class also.</p>


<H3>Important: Getting started in Python</H3>

<p><b>Coding prerequisities:</b> Prior coding experience is not required for this class, but students without Python experience will need to complete a crash course in the language. In particular, I have made video lectures covering Python basics which are available via Canvas for anyone with a Princeton netid. To view the videos, please <a href="https://princeton.instructure.com/enroll/KB43PD">self-enroll here.</a><br><br> The first several homeworks will assume that you have watched certain portions of the crash course, so you can do this concurrently with the start of classes, <b>however</b> if you are new to coding I <b>strongly recommend</b> getting started <b>before the beginning of the semester</b> so you can work at your own pace and do additional practice as needed. If you fall behind on this once the semester starts, it may be prohibitively difficult to catch up. The videos have accompanying code notebooks with exercises. If you can complete the exercises in the notebooks, that means you are on a good track. Students with prior experience in Python should test their understanding by making sure they can easily complete those exercises.</p>

<p>Please note that the video tutorial does not cover all aspects of Python; rather the focus is on the key aspects of the language required for the course.</p>

<p><b>Additional resources:</b> A repository of good tutorials can be found <a href="https://researchcomputing.princeton.edu/external-online-resources/python">here</a>. For the <a href="https://www.w3schools.com/python/">W3 Schools tutorial,</a> You might also take a look at <a href="https://researchcomputing.princeton.edu/learn/workshops-live-training">Princeton Research Computing's offerings</a>, as they feature mini-courses on Python at various times throughout the year.</p>

<H3>Schedule of Topics</H3>

<p><b>Week 1:</b> Overview of machine learning<br>
Reading: Chollet, chapter 1</p>

<p><b>Week 2:</b> Fundamentals of neural networks I: The forward pass<br>
Reading: Chollet & Watson, sections 2.1-2.3</p>

<p><b>Week 3:</b> Fundamentals of neural networks II: Backpropagation <br>
Reading: Chollet & Watson, 2.4-end of chapter </p>

<p><b>Week 4:</b>  Classification and regression<br>
Reading: Softmax Tutorial and Tasks, Activations, and Losses posted to Canvas. Optional reading on deeper dive on softmax/sigmoid available on Canvas.</p>

<p><b>Week 5:</b> Introduction to Keras; Training, validation, and test sets<br>
Reading: Chollet & Watson, sections 3.1, 3.2, 3.6 (skipping 3.6.1) & 4.1-4.2; section 7.2.2</p>

<p><b>Week 6:</b> Finish any outstanding topics; Review; Begin computer vision, time permitting<br> 
Reading: Chollet & Watson, sections 5.2-5.4; chapter 6 (optional) <br></p>
	
<em>Classification/regression project assignment distributed</em>

<p>Spring break</p>

<p><b>Week 7:</b> Computer vision I: Introduction to convolutional neural networks<br>
Reading: Chollet & Watson, sections 8.1-8.2; Tutorial on RBG images posted to Canvas<br></p>


<em>Classification/regression project due</em>

<p><b>Week 8:</b> Computer vision II: classification; dropout regularization; visualizing convnets<br>
Reading: Chollet & Watson, 8.3;  Reading/Tutorial on leveraging pretrained convnets and on structuring folders in Python posted to Canvas.<br></p>


<em>Computer vision project assignment distributed</em>


<p><b>Week 9:</b> Introduction to natural language processing: Representing words as numbers<br>
Reading: Reading/tutorial posted to Canvas; <a href="http://jalammar.github.io/illustrated-word2vec/">Jay Alammar, Illustrated Word2Vec</a>; Chollet & Watson, 14.5.2, 14.5.4-end<br> Optional: rest of Chollet & Watson chapter 14. Optional social science application of word embeddings posted to Canvas.<br></p>



<p><b>Week 10:</b> Transformers I: Introduction; Masked language models<br>
Reading: <a href="http://jalammar.github.io/illustrated-transformer/">Jay Alammar, The Illustrated Transformer</a>, Chollet & Watson sections 15.3-15.3.2, 15.3.5, 15.5<br></p>

<em>Computer vision project due</em><br>


<p><b>Week 11:</b> Transformers II: Generative models<br>
Reading: Chollet & Watson chapter 15.3.3<br></p>


<em>NLP project distributed</em><br>

<p><b>Week 12:</b> Transformers III: Generative models cont.; Putting it all together: Review and loose ends<br>
Optional Reading: Chollet & Watson, chapter 19<br></p>

<em>NLP project due; exact date TBD</em>

<br>
<br>

<p>Below is an eclectic collection of supplemental readings that may be of interest as we go along, particularly in the second half of the semester.</p>

<a href="https://arxiv.org/abs/2207.07048">Kapoor, S. & Narayanan, A. (2022)</a> <br>
<a href="https://academic.oup.com/dsh/article/35/1/194/5296356">Melvin Wevers, Thomas Smits, The visual digital turn: Using neural networks to study historical images, Digital Scholarship in the Humanities, Volume 35, Issue 1, April 2020, Pages 194–207.</a><br>
<a href="https://www.sciencedirect.com/science/article/pii/S0305440321000455">Leszek M. Pawlowicz, Christian E. Downum, Applications of deep learning to decorated ceramic typology and classification: A case study using Tusayan White Ware from Northeast Arizona, Journal of Archaeological Science, Volume 130, 2021, 105375.</a></p>
<a href="https://tessaescharlesworth.files.wordpress.com/2022/07/charlesworth_hist-embeddings_published.pdf">Charlesworth, T.E.S., Caliskan, A., & Banaji, M.R., (2022) Historical representations of social groups across 200 years of word embeddings from Google Books. Proceedings of the National Academy of Sciences, 119(28).</a><br> 
<a href="https://www.nature.com/articles/s41562-022-01316-8">Grand, G., Blank, I.A., Pereira, F. et al. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. Nat Hum Behav 6, 975–987 (2022).</a> <br>
<a href="https://www.nature.com/articles/s41586-022-04448-z?utm_campaign=The%20Batch&utm_medium=email&_hsmi=222428230&_hsenc=p2ANqtz-_kr0H_rGJOIJRXWpDlGwP298BuR5SoKbROhB05hKXLpwXYaktKDQq3fq7RfNIXxV4DSCytHhkc_T2aCIlnx-SWBivzg1GfDrQFM5c4bz-0KgE_Low&utm_content=222428230&utm_source=hs_email">Assael, Y., Sommerschield, T., Shillingford, B. et al. Restoring and attributing ancient texts using deep neural networks. Nature 603, 280–283 (2022).</a> <br>
<a href="https://www.pnas.org/doi/10.1073/pnas.1907367117">Manning, C., Clark, K., Hewitt, J., & Levy, O. Emergent linguistic structure in artificial neural networks trained by self-supervision. PNAS 117 (48).</a><br>
<a href="https://www.scientificamerican.com/article/we-asked-gpt-3-to-write-an-academic-paper-about-itself-mdash-then-we-tried-to-get-it-published/">Osmanovic Thunström, A. (2022). We Asked GPT-3 to Write an Academic Paper about Itself—Then We Tried to Get It Published. Scientific American, June 30 2022.</a> <br>
<a href="https://hal.archives-ouvertes.fr/hal-03701250/document"> Gpt Generative Pretrained Transformer, Almira Osmanovic Thunström, Steinn Steingrimsson. Can GPT-3 write an academic paper on itself, with minimal human input?. 2022. hal-03701250.</a><br>
<a href="https://www.nature.com/articles/d41586-021-00530-0">Hutson, M. (2021). Robo-writers: the rise and risks of language-generating AI. Nature 591, 22-25.</a><br>
<a href="https://www.nature.com/articles/d41586-023-00340-6">Stokel-Walker & Van Noorden (2023). What ChatGPT and generative AI mean for science. Nature.</a><br>
<a href="https://www.nature.com/articles/d41586-023-00528-w"> Tregoning, J. (2023). AI writing tools could give scientists the gift of time. Nature.</a><br>
<a href="https://www.nature.com/articles/d41586-023-00288-7">van Dis, E. et al. (2023). ChatGPT: Five priorities for research. Nature.</a><br>
<a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/98F5E24BEADE585050B773D2CBEB1F39/S0045509121000230a.pdf/whats_wrong_with_automated_influence.pdf">Benn, C., & Lazar, S. (2022). What’s wrong with Automated Influence. Canadian Journal of Philosophy, 1-24.</a></p>



