---
layout: page
title: "SML 543"
permalink: /SML543
---

## Machine Learning: A practical introduction for humanists and social scientists

<b>Draft syllabus - Spring 2024</b>


Sarah-Jane Leslie<br>
sjleslie@princeton.edu<br>
	<a href="www.princeton.edu/leslie">www.princeton.edu/leslie</a>




<p>Machine learning – and in particular, deep learning – is rapidly opening new horizons for research in the humanities and social sciences. However, scholars in the humanities and social sciences can encounter barriers to learning about such techniques – for example, machine learning courses, especially at the graduate level, often require multivariate calculus, linear algebra and prior coding experience, which students in the humanities and less quantitative social sciences may lack. This course offers a practical introduction to deep learning for graduate students, without assuming knowledge of calculus or other college-level math, or any prior experience with coding. By the end of the course, students will:</p> 

<ul><li>be able to code and train a variety of basic deep learning models</li>
<li>develop an appreciation of the range of humanities/social science research questions to which deep learning can be applied</li>
<li>be fluent collaborators on research projects that involve machine learning experts</li>
<li>gain an understanding than will inform theorizing about machine learning (e.g., for research in AI ethics, technology policy, etc)</li></ul>

<p><b>Readings:</b> The primary text for the course is <em>Francois Chollet, 2021, Deep learning with Python. Manning.</em> We will work our way through the book in a lot of detail. In the second half of the course, once we have worked through the basics, there are supplemental readings each week, which give examples of how related models have been used in humanities/social science research. The supplemental readings aim to provide a sense of the breadth of applications, as well as to raise interesting issues for further thought and consideration.</p>

<p><b>Enrollment eligibility: </b>Open only to Princeton graduate students, except with special permission of the instructor. Princeton undergraduates should take the undergraduate version of this course, <a href="/SML354">SML 354</a></p>

<p><b>Evaluation and assignments:</b> There are simple weekly coding assignments to cement knowledge and build skills. Students taking the course for credit are required to complete these, though they are ungraded. That is, as long as a reasonable effort is made on them, students receive full credit for the week. In addition, there are three graded projects spaced throughout the semester. For these projects, students will be supplied with a data set and asked to build a model that addresses a question appropriate to the data set. Evaluation will be based on the appropriateness of the model for the question and the success of the code used to build/train the model, as well as answers to any other questions posed along the way. The three projects will cover the following areas respectively: classification/regression, computer vision, natural language processing. (For philosophy students only: successful completion of the above assignments will constitute a logic unit. Philosophy students wishing to receive a unit other than logic should contact me directly.) </p> 

<p>Final grades are determined in the following way: 20% homework, 20% classification/regression assignment, 30% computer vision assignment, 30% natural language processing assignment. </p>

<p><b>Auditing:</b> Graduate students are welcome to audit this course, however I request that even auditors engage with the weekly coding assignments, since the value of this class is largely associated with the development of practical skills. As with any kind of coding, these skills can only be learned by doing. Post-docs and other researchers/faculty are welcome to informally audit the class also.</p>


<H3>Important: Getting started in Python</H3>

<p>Prior coding experience is not required for this class, but students without Python experience will need to complete a crash course in the language. In particular, I have made video lectures covering Python basics which will be available via Canvas upon registering for the class. The first several homeworks will assume that you have watched certain portions of the crash course, so you can do this concurrently with the start of classes, <b>however</b> if you are new to coding I <b>strongly recommend</b> getting started in January so you can work at your own pace and do additional practice as needed. If you fall behind on this once the semester starts, it may be prohibitively difficult to catch up. The videos have accompanying code notebooks with exercises. If you can complete the exercises in the notebooks, that means you are on a good track. Students with prior experience in Python should test their understanding by making sure they can easily complete those exercises.</p>

<p>Please note that the video tutorial does not cover all aspects of Python; rather the focus is on the key aspects of the language required for the course.</p>

<p><b>Additional resources:</b> A repository of good tutorials can be found <a href="https://researchcomputing.princeton.edu/external-online-resources/python">here</a>. For the <a href="https://www.w3schools.com/python/">W3 Schools tutorial,</a> I recommend working through it up to Classes/Objects. You might also consider attending a Wintersession course on Python.</p>

<H3>Schedule of Topics</H3>

<p><b>Week 1:</b> Overview of machine learning<br>
Reading: Chollet, chapter 1</p>

<p><b>Week 2:</b> Fundamentals of neural networks: the forward pass<br>
Reading: Chollet, chapter 2</p>

<p><b>Week 3:</b> Fundamentals of neural networks: the backward pass<br>
Reading: Chollet, chapter 2 cont.</p>

<p><b>Week 4:</b> Introduction to Keras;  Classification and regression<br>
Reading: Chollet, sections 3.1-3.3, 3.6, & 4.1-4.2</p>

<p><b>Week 5:</b> Training, validation, and test sets; regularization; other practicalities<br>
Reading: Chollet, section 4.3 & chapter 5</p>

<p><b>Week 6:</b> Best practices for model evaluation; The Keras functional API<br>
Reading: Chollet, chapter 6 & chapter 7 up to (but not including) section 7.2.3<br> 
	<a href="https://arxiv.org/abs/2207.07048">Kapoor, S. & Narayanan, A. (2022)</a> </p>

<em>Classification/regression project assignment distributed</em>

<p>Spring break</p>

<p><b>Week 7:</b> Computer vision I: Introduction to convolutional neural networks<br>
Reading: Chollet, sections 8.1-8.2<br>
Supplemental reading: <a href="https://academic.oup.com/dsh/article/35/1/194/5296356">Melvin Wevers, Thomas Smits, The visual digital turn: Using neural networks to study historical images, Digital Scholarship in the Humanities, Volume 35, Issue 1, April 2020, Pages 194–207.</a></p>

<em>Classification/regression project due</em>

<p><b>Week 8:</b> Computer vision II: Classification; Leveraging pretrained models<br>
Reading: Chollet, sections 8.3; 9.3<br>
Supplemental reading: <a href="https://www.sciencedirect.com/science/article/pii/S0305440321000455">Leszek M. Pawlowicz, Christian E. Downum, Applications of deep learning to decorated ceramic typology and classification: A case study using Tusayan White Ware from Northeast Arizona, Journal of Archaeological Science, Volume 130, 2021, 105375.</a></p>

<em>Computer vision project assignment distributed</em>

<p><b>Week 9:</b> Natural Language Processing I: Representing words as numbers<br>
Reading: Chollet sections 11.1-11.3<br>
Supplemental reading: <a href="https://tessaescharlesworth.files.wordpress.com/2022/07/charlesworth_hist-embeddings_published.pdf">Charlesworth, T.E.S., Caliskan, A., & Banaji, M.R., (2022) Historical representations of social groups across 200 years of word embeddings from Google Books. Proceedings of the National Academy of Sciences, 119(28).</a><br> 
<a href="https://www.nature.com/articles/s41562-022-01316-8">Grand, G., Blank, I.A., Pereira, F. et al. Semantic projection recovers rich human knowledge of multiple object features from word embeddings. Nat Hum Behav 6, 975–987 (2022).</a> <br>


<p><b>Week 10:</b> NLP II: Introduction to Transformers; Masked Language Models<br>
Reading: Chollet chapter 11.4<br>
Supplemental reading: <a href="https://www.nature.com/articles/s41586-022-04448-z?utm_campaign=The%20Batch&utm_medium=email&_hsmi=222428230&_hsenc=p2ANqtz-_kr0H_rGJOIJRXWpDlGwP298BuR5SoKbROhB05hKXLpwXYaktKDQq3fq7RfNIXxV4DSCytHhkc_T2aCIlnx-SWBivzg1GfDrQFM5c4bz-0KgE_Low&utm_content=222428230&utm_source=hs_email">Assael, Y., Sommerschield, T., Shillingford, B. et al. Restoring and attributing ancient texts using deep neural networks. Nature 603, 280–283 (2022).</a> </br>
<a href="https://www.pnas.org/doi/10.1073/pnas.1907367117">Manning, C., Clark, K., Hewitt, J., & Levy, O. Emergent linguistic structure in artificial neural networks trained by self-supervision. PNAS 117 (48).</a></p>

<em>Computer vision project due</em><br>

<p><b>Week 11:</b> NLP III: Generative models<br>
Reading: Chollet chapter 11.5, 12.1<br>
Supplemental reading: <a href="https://www.scientificamerican.com/article/we-asked-gpt-3-to-write-an-academic-paper-about-itself-mdash-then-we-tried-to-get-it-published/">Osmanovic Thunström, A. (2022). We Asked GPT-3 to Write an Academic Paper about Itself—Then We Tried to Get It Published. Scientific American, June 30 2022.</a> <br>
<a href="https://hal.archives-ouvertes.fr/hal-03701250/document"> Gpt Generative Pretrained Transformer, Almira Osmanovic Thunström, Steinn Steingrimsson. Can GPT-3 write an academic paper on itself, with minimal human input?. 2022. hal-03701250.</a><br>
<a href="https://www.nature.com/articles/d41586-021-00530-0">Hutson, M. (2021). Robo-writers: the rise and risks of language-generating AI. Nature 591, 22-25.</a></p>
<p>Newly added: <a href="https://www.nature.com/articles/d41586-023-00340-6">Stokel-Walker & Van Noorden (2023). What ChatGPT and generative AI mean for science. Nature.</a><br>
	<a href="https://www.nature.com/articles/d41586-023-00528-w"> Tregoning, J. (2023). AI writing tools could give scientists the gift of time. Nature.</a><br>
	<a href="https://www.nature.com/articles/d41586-023-00288-7">van Dis, E. et al. (2023). ChatGPT: Five priorities for research. Nature.</a>
</p>

<em>NLP project distributed</em><br>

<p><b>Week 12:</b> Discussion: AI in society; Review and loose ends<br>
Supplemental readings: <a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-like-people/A9535B1D745A0377E16C590E14B94993">Lake, B., Ullman, T., Tenenbaum, J., & Gershman, S. (2017). Building machines that learn and think like people. Behavioral and Brain Sciences, 40, E253.</a> <br>
	<a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/98F5E24BEADE585050B773D2CBEB1F39/S0045509121000230a.pdf/whats_wrong_with_automated_influence.pdf">Benn, C., & Lazar, S. (2022). What’s wrong with Automated Influence. Canadian Journal of Philosophy, 1-24.</a></p>


<em>NLP project due on Dean's date</em>
