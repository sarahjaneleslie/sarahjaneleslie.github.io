
<b>DRAFT SYLLABUS - Spring 2024</b>

<H1>PHI 354/SML 354<br>
Artificial Intelligence: A hands-on introduction from basics to ChatGPT</H1>
<H3>Sarah-Jane Leslie<br>
sjleslie@princeton.edu<br>
	<a href="www.princeton.edu/leslie">www.princeton.edu/leslie</a></H3>

<h1>TENTATIVE AND SUBJECT TO FURTHER CHANGES; PLEASE CHECK BACK SOON</h1>

<p>Articial intelligence is rapidly transforming the world we live in, but gaining literacy with it can be challenging for students without strong mathematical and computational backgrounds. This course offers an introduction to deep learning, which is the core technology behind most modern AI applications such as ChatGPT, aimed at students with little to no coding experience and no mathematical background beyond basic differential calculus. Emphasis will be placed on gaining a conceptual understanding of deep learning models, and on gaining the basic coding skills required to use them in simple contexts. By the end of the course, students will be able to understand, code and train a variety of basic deep learning models, including basic neural nets, image recognition models, and natural language processing models. As a capstone, students will complete a structured project in which they will build their own (tiny) GPT-style text generator.</p>


<p>Please note: Students who satisfy the MAT and COS requirements for COS 324: Introduction to Machine Learning should take that course instead. No credit will be given to students who have already completed COS 324 or equivalent. Please check with the instructor if you have any questions about your eligibility. </p>

<p><b>Readings:</b> The primary text for the course is <em>Francois Chollet, 2021, Deep learning with Python. Manning.</em> We will work our way through the book in a lot of detail. In the second half of the course, once we have worked through the basics, there will be supplemental readings assigned as applicable</p>

<p><b>Evaluation and assignments:</b> Students are required to complete weekly coding assignments and a final structured  project (coding and training a tiny GPT-style text generator). In addition, there are in-class midterm and final exams that test conceptual understanding of course content.</p> 

<p>Final grades are determined in the following way: 25% homeworks, 25% midterm, 25% final, 15% final project, 10% class/precept participation. </p>


<H3>Important: Getting started in Python</H3>

<p>Prior coding experience is not required for this class, but students without Python experience will need to complete a crash course in the language. In particular, I have made video lectures covering Python basics which will be available via Canvas upon registering for the class. The first several homeworks will assume that you have watched certain portions of the crash course, so you can do this concurrently with the start of classes, <b>however</b> if you are new to coding I strongly recommend getting started in January so you can work at your own pace and so additional practice as needed. The videos have accompanying code notebooks with exercises. If you can complete the exercises in the notebooks, that means you are on a good track. Students with prior experience in Python should test their understanding by making sure they can easily complete those exercises.</p>

<p>Please note that the video tutorial does not cover all aspects of Python; rather the focus is on the key aspects of the language required for the course.</p>

<p><b>Additional resources:</b> A repository of good tutorials can be found <a href="https://researchcomputing.princeton.edu/external-online-resources/python">here</a>. For the <a href="https://www.w3schools.com/python/">W3 Schools tutorial,</a> I recommend working through it up to Classes/Objects. You might also consider attending a Wintersession course on Python.</p>


<p>To get started writing Python code, you first need to <a href="https://forms.rc.princeton.edu/registration/?q=adroit">create an Adroit account.</a> Once you have done so, you can simply go <a href="https://myadroit.princeton.edu/">here</a> and navigate to Interactive Apps then Jupyter for Classes. (Note that VPN is required from off campus.) You do not need to install anything locally on your computer. More detailed instructions will be available on the Canvas page for this course.</p>

<H3>Schedule of Topics -- updated 2/17</H3>

<p><b>Week 1:</b> Overview of machine learning; crash course in Python basics<br>
Reading: Chollet, chapter 1</p>

<p><b>Week 2:</b> Fundamentals of neural networks: the forward pass<br>
Reading: Chollet, chapter 2</p>

<p><b>Week 3:</b> Fundamentals of neural networks: the backward pass<br>
Reading: Chollet, chapter 2 cont.</p>

<p><b>Week 4:</b> Introduction to Keras;  Classification and regression<br>
Reading: Chollet, sections 3.1-3.3, 3.6, & 4.1-4.2</p>

<p><b>Week 5:</b> Training, validation, and test sets; regularization; other practicalities<br>
Reading: Chollet, section 4.3 & chapter 5</p>

<p><b>Week 6:</b> Best practices for model evaluation; overview of other helpful Python libraries<br>
Reading: Chollet, chapter 6, <a href="https://arxiv.org/abs/2207.07048">Kapoor, S. & Narayanan, A. (2022)</a> </p>


<p>break</p>

<p><b>Week 7:</b> The Keras functional API; loose ends; introduction to computer vision<br>
Reading: Chollet, chapter 7 up to (but not including) section 7.2.3<br>
Supplemental reading:  </p>



<p><b>Week 8:</b> Computer vision I: introduction to convolutional neural networks<br>
Reading: Chollet, sections 8.1-8.2<br>


<p><b>Week 9:</b> Computer vision II: classification; pretrained models<br>
Reading: Chollet, sections 8.3; 9.3<br>


<p><b>Week 10:</b> Natural Language Processing I: representing words as numbers<br>
Reading: Chollet sections 11.1-11.3<br>
Supplemental reading: <a href="https://tessaescharlesworth.files.wordpress.com/2022/07/charlesworth_hist-embeddings_published.pdf">Charlesworth, T.E.S., Caliskan, A., & Banaji, M.R., (2022) Historical representations of social groups across 200 years of word embeddings from Google Books. Proceedings of the National Academy of Sciences, 119(28).</a><br> 




<p><b>Week 11:</b> NLP II: transformers<br>
Reading: Chollet chapter 11.4<br>
Supplemental reading:
<a href="https://www.pnas.org/doi/10.1073/pnas.1907367117">Manning, C., Clark, K., Hewitt, J., & Levy, O. Emergent linguistic structure in artificial neural networks trained by self-supervision. PNAS 117 (48).</a></p>



<p><b>Week 12:</b> NLP III: generative models; loose ends<br>
Reading: Chollet chapter 11.5, 12.1<br>
Supplemental reading: <a href="https://www.scientificamerican.com/article/we-asked-gpt-3-to-write-an-academic-paper-about-itself-mdash-then-we-tried-to-get-it-published/">Osmanovic Thunström, A. (2022). We Asked GPT-3 to Write an Academic Paper about Itself—Then We Tried to Get It Published. Scientific American, June 30 2022.</a> <br>
<a href="https://hal.archives-ouvertes.fr/hal-03701250/document"> Gpt Generative Pretrained Transformer, Almira Osmanovic Thunström, Steinn Steingrimsson. Can GPT-3 write an academic paper on itself, with minimal human input?. 2022. hal-03701250.</a><br>
<a href="https://www.nature.com/articles/d41586-021-00530-0">Hutson, M. (2021). Robo-writers: the rise and risks of language-generating AI. Nature 591, 22-25.</a></p>
<p>Newly added: <a href="https://www.nature.com/articles/d41586-023-00340-6">Stokel-Walker & Van Noorden (2023). What ChatGPT and generative AI mean for science. Nature.</a><br>
	<a href="https://www.nature.com/articles/d41586-023-00528-w"> Tregoning, J. (2023). AI writing tools could give scientists the gift of time. Nature.</a><br>
	<a href="https://www.nature.com/articles/d41586-023-00288-7">van Dis, E. et al. (2023). ChatGPT: Five priorities for research. Nature.</a>
</p>

Additional supplemental readings: <a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-like-people/A9535B1D745A0377E16C590E14B94993">Lake, B., Ullman, T., Tenenbaum, J., & Gershman, S. (2017). Building machines that learn and think like people. Behavioral and Brain Sciences, 40, E253.</a> <br>
	<a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/98F5E24BEADE585050B773D2CBEB1F39/S0045509121000230a.pdf/whats_wrong_with_automated_influence.pdf">Benn, C., & Lazar, S. (2022). What’s wrong with Automated Influence. Canadian Journal of Philosophy, 1-24.</a></p>


<em>NLP project due on Dean's date: May 9th</em>
