
<b>DRAFT SYLLABUS - Spring 2024</b>

<H1>PHI 354/SML 354<br>
Artificial Intelligence: A hands-on introduction from basics to ChatGPT</H1>
<H3>Sarah-Jane Leslie<br>
sjleslie@princeton.edu<br>
	<a href="www.princeton.edu/leslie">www.princeton.edu/leslie</a></H3>


<p>Articial intelligence is rapidly transforming the world we live in, but gaining literacy with it can be challenging for students without strong mathematical and computational backgrounds. This course offers an introduction to deep learning, which is the core technology behind most modern AI applications such as ChatGPT, aimed at students with little to no coding experience and no mathematical background beyond basic differential calculus. 
 By the end of the course, students will be able to understand, code and train a variety of basic deep learning models, including:</p> 

<ul><li>basic neural nets</li>
<li>image recognition models</li>
<li>natural language processing models, including text generation models</li>
</ul>

<p>Please note: Students who satisfy the requirements for COS 324: Introduction to Machine Learning should take that course instead. No credit will be given to students who have already completed COS 324 or equivalent. Please check with the instructor if you have any questions about your eligibility. </p>

<p><b>Readings:</b> The primary text for the course is <em>Francois Chollet, 2021, Deep learning with Python. Manning.</em> We will work our way through the book in a lot of detail. In the second half of the course, once we have worked through the basics, there will be supplemental readings assigned as needed.</p>

<p><b>Evaluation and assignments:</b> TBA </p> 

<p>Final grades are determined in the following way: TBA </p>


<H3>Important: Getting started in Python</H3>

<p>I <b>highly recommend</b> that students who have never coded before do a Python tutorial prior to the first class. A repository of good tutorials can be found <a href="https://researchcomputing.princeton.edu/external-online-resources/python">here</a>. For the <a href="https://www.w3schools.com/python/">W3 Schools tutorial,</a> I recommend working through it up to Classes/Objects. You might also consider attending a 2023 Wintersession course such as Intro to Programming Using Python or Python for Poets.</p>

<p>I also recommend taking a look at <a href="https://pythontutor.com/python-debugger.html#mode=edit">this resource</a> for visualizing and understanding the execution of programs in Python.</p>

<p>We will spend much of the first session doing a crash course in Python, but coding is a skill that takes time to develop. The first class will be very difficult for you if you are seeing the material for the very first time.</p>

<p>To get started writing Python code, you first need to <a href="https://forms.rc.princeton.edu/registration/?q=adroit">create an Adroit account.</a> Once you have done so, you can simply go <a href="https://myadroit.princeton.edu/">here</a> and navigate to Interactive Apps then Jupyter for Classes. (Note that VPN is required from off campus.) You do not need to install anything locally on your computer. More detailed instructions can be found on the Canvas page for this course.</p>

<H3>Schedule of Topics -- updated 2/17</H3>

<p><b>Week 1:</b> Overview of machine learning; crash course in Python basics<br>
Reading: Chollet, chapter 1</p>

<p><b>Week 2:</b> Fundamentals of neural networks: the forward pass<br>
Reading: Chollet, chapter 2</p>

<p><b>Week 3:</b> Fundamentals of neural networks: the backward pass<br>
Reading: Chollet, chapter 2 cont.</p>

<p><b>Week 4:</b> Introduction to Keras;  Classification and regression<br>
Reading: Chollet, sections 3.1-3.3, 3.6, & 4.1-4.2</p>

<p><b>Week 5:</b> Training, validation, and test sets; regularization; other practicalities<br>
Reading: Chollet, section 4.3 & chapter 5</p>

<p><b>Week 6:</b> Best practices for model evaluation; overview of other helpful Python libraries<br>
Reading: Chollet, chapter 6, <a href="https://arxiv.org/abs/2207.07048">Kapoor, S. & Narayanan, A. (2022)</a> </p>


<p>break</p>

<p><b>Week 7:</b> The Keras functional API; loose ends; introduction to computer vision<br>
Reading: Chollet, chapter 7 up to (but not including) section 7.2.3<br>
Supplemental reading:  </p>



<p><b>Week 8:</b> Computer vision I: introduction to convolutional neural networks<br>
Reading: Chollet, sections 8.1-8.2<br>


<p><b>Week 9:</b> Computer vision II: classification; pretrained models<br>
Reading: Chollet, sections 8.3; 9.3<br>


<p><b>Week 10:</b> Natural Language Processing I: representing words as numbers<br>
Reading: Chollet sections 11.1-11.3<br>
Supplemental reading: <a href="https://tessaescharlesworth.files.wordpress.com/2022/07/charlesworth_hist-embeddings_published.pdf">Charlesworth, T.E.S., Caliskan, A., & Banaji, M.R., (2022) Historical representations of social groups across 200 years of word embeddings from Google Books. Proceedings of the National Academy of Sciences, 119(28).</a><br> 




<p><b>Week 11:</b> NLP II: transformers<br>
Reading: Chollet chapter 11.4<br>
Supplemental reading:
<a href="https://www.pnas.org/doi/10.1073/pnas.1907367117">Manning, C., Clark, K., Hewitt, J., & Levy, O. Emergent linguistic structure in artificial neural networks trained by self-supervision. PNAS 117 (48).</a></p>



<p><b>Week 12:</b> NLP III: generative models; loose ends<br>
Reading: Chollet chapter 11.5, 12.1<br>
Supplemental reading: <a href="https://www.scientificamerican.com/article/we-asked-gpt-3-to-write-an-academic-paper-about-itself-mdash-then-we-tried-to-get-it-published/">Osmanovic Thunström, A. (2022). We Asked GPT-3 to Write an Academic Paper about Itself—Then We Tried to Get It Published. Scientific American, June 30 2022.</a> <br>
<a href="https://hal.archives-ouvertes.fr/hal-03701250/document"> Gpt Generative Pretrained Transformer, Almira Osmanovic Thunström, Steinn Steingrimsson. Can GPT-3 write an academic paper on itself, with minimal human input?. 2022. hal-03701250.</a><br>
<a href="https://www.nature.com/articles/d41586-021-00530-0">Hutson, M. (2021). Robo-writers: the rise and risks of language-generating AI. Nature 591, 22-25.</a></p>
<p>Newly added: <a href="https://www.nature.com/articles/d41586-023-00340-6">Stokel-Walker & Van Noorden (2023). What ChatGPT and generative AI mean for science. Nature.</a><br>
	<a href="https://www.nature.com/articles/d41586-023-00528-w"> Tregoning, J. (2023). AI writing tools could give scientists the gift of time. Nature.</a><br>
	<a href="https://www.nature.com/articles/d41586-023-00288-7">van Dis, E. et al. (2023). ChatGPT: Five priorities for research. Nature.</a>
</p>

Additional supplemental readings: <a href="https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-like-people/A9535B1D745A0377E16C590E14B94993">Lake, B., Ullman, T., Tenenbaum, J., & Gershman, S. (2017). Building machines that learn and think like people. Behavioral and Brain Sciences, 40, E253.</a> <br>
	<a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/98F5E24BEADE585050B773D2CBEB1F39/S0045509121000230a.pdf/whats_wrong_with_automated_influence.pdf">Benn, C., & Lazar, S. (2022). What’s wrong with Automated Influence. Canadian Journal of Philosophy, 1-24.</a></p>


<em>NLP project due on Dean's date: May 9th</em>
